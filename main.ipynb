{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Info Checker - Analyseur de Cr√©dibilit√© TikTok\n",
        "\n",
        "Ce notebook permet d'analyser et v√©rifier la cr√©dibilit√© des informations partag√©es par les influenceurs TikTok.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration et Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Ajouter le dossier src au path\n",
        "sys.path.append(str(Path.cwd()))\n",
        "\n",
        "from src.config import Config\n",
        "from src.downloader import TikTokDownloader\n",
        "from src.transcriber import AudioTranscriber\n",
        "from src.analyzer import LLMAnalyzer\n",
        "from src.fact_checker import FactChecker\n",
        "from src.visualizer import ResultVisualizer\n",
        "from src.storage import ResultStorage\n",
        "\n",
        "# Valider la configuration\n",
        "try:\n",
        "    Config.validate()\n",
        "    print(\"‚úÖ Configuration valid√©e\")\n",
        "except ValueError as e:\n",
        "    print(f\"‚ùå Erreur de configuration: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Param√®tres d'Analyse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choisir le mode d'analyse\n",
        "analysis_mode = \"video\"  # \"video\" ou \"user\"\n",
        "\n",
        "# Pour une vid√©o sp√©cifique\n",
        "video_url = \"\"  # Exemple: \"https://www.tiktok.com/@username/video/1234567890\"\n",
        "\n",
        "# Pour un utilisateur complet\n",
        "username = \"\"  # Exemple: \"username\" (sans @)\n",
        "max_videos = 5  # Nombre maximum de vid√©os √† analyser\n",
        "\n",
        "# Configuration LLM\n",
        "llm_provider = Config.DEFAULT_LLM_PROVIDER  # \"openai\", \"anthropic\", ou \"local\"\n",
        "\n",
        "# Langue\n",
        "language = \"fr\"\n",
        "\n",
        "print(f\"Mode d'analyse: {analysis_mode}\")\n",
        "print(f\"Provider LLM: {llm_provider}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. T√©l√©chargement des Vid√©os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "downloader = TikTokDownloader()\n",
        "video_paths = []\n",
        "video_metadata_list = []\n",
        "\n",
        "if analysis_mode == \"video\":\n",
        "    if not video_url:\n",
        "        print(\"‚ö†Ô∏è Veuillez fournir une URL de vid√©o\")\n",
        "    else:\n",
        "        print(f\"üì• T√©l√©chargement de la vid√©o: {video_url}\")\n",
        "        # R√©cup√©rer les m√©tadonn√©es d'abord\n",
        "        metadata = downloader.get_video_info(video_url)\n",
        "        video_metadata_list.append(metadata)\n",
        "        \n",
        "        # T√©l√©charger la vid√©o\n",
        "        video_path = downloader.download_video(video_url)\n",
        "        video_paths.append(video_path)\n",
        "        print(f\"‚úÖ Vid√©o t√©l√©charg√©e: {video_path.name}\")\n",
        "\n",
        "elif analysis_mode == \"user\":\n",
        "    if not username:\n",
        "        print(\"‚ö†Ô∏è Veuillez fournir un nom d'utilisateur\")\n",
        "    else:\n",
        "        print(f\"üì• T√©l√©chargement des vid√©os de @{username}...\")\n",
        "        video_paths = downloader.download_user_videos(username, max_videos)\n",
        "        # Cr√©er des m√©tadonn√©es vides pour chaque vid√©o\n",
        "        video_metadata_list = [{}] * len(video_paths)\n",
        "        print(f\"‚úÖ {len(video_paths)} vid√©o(s) t√©l√©charg√©e(s)\")\n",
        "\n",
        "print(f\"\\nTotal de vid√©os √† analyser: {len(video_paths)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transcription des Vid√©os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transcriber = AudioTranscriber(model_size=\"base\")  # \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
        "transcriptions = []\n",
        "\n",
        "for i, video_path in enumerate(video_paths, 1):\n",
        "    print(f\"\\nüé§ Transcription {i}/{len(video_paths)}: {video_path.name}\")\n",
        "    transcription = transcriber.transcribe_video(video_path, language=language)\n",
        "    transcription['video_path'] = str(video_path)\n",
        "    transcriptions.append(transcription)\n",
        "    print(f\"‚úÖ Transcription termin√©e ({len(transcription['text'])} caract√®res)\")\n",
        "    print(f\"üìù Extrait: {transcription['text'][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Analyse LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyzer = LLMAnalyzer(provider=llm_provider)\n",
        "llm_analyses = []\n",
        "\n",
        "# S'assurer que video_metadata_list a la m√™me longueur que transcriptions\n",
        "while len(video_metadata_list) < len(transcriptions):\n",
        "    video_metadata_list.append({})\n",
        "\n",
        "for i, (transcription, metadata) in enumerate(zip(transcriptions, video_metadata_list), 1):\n",
        "    print(f\"\\nü§ñ Analyse LLM {i}/{len(transcriptions)}\")\n",
        "    \n",
        "    # Analyser\n",
        "    analysis = analyzer.analyze_content(\n",
        "        transcription=transcription['text'],\n",
        "        video_metadata=metadata\n",
        "    )\n",
        "    \n",
        "    llm_analyses.append(analysis)\n",
        "    print(f\"‚úÖ Analyse termin√©e\")\n",
        "    print(f\"üìä Extrait: {analysis['analysis'][:300]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Extraction des Affirmations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extraire les affirmations depuis l'analyse LLM\n",
        "import re\n",
        "\n",
        "def extract_claims_from_analysis(analysis_text: str) -> list:\n",
        "    \"\"\"Extrait les affirmations de l'analyse LLM\"\"\"\n",
        "    patterns = [\n",
        "        r'(?:affirme|dit|pr√©tend|soutient|d√©clare|assure)[^.]*\\.',\n",
        "        r'(?:selon|d\\'apr√®s|selon les)[^.]*\\.',\n",
        "    ]\n",
        "    \n",
        "    claims = []\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, analysis_text, re.IGNORECASE)\n",
        "        claims.extend(matches)\n",
        "    \n",
        "    if not claims:\n",
        "        sentences = re.split(r'[.!?]+', analysis_text)\n",
        "        claims = [s.strip() for s in sentences if len(s.strip()) > 20][:5]\n",
        "    \n",
        "    return claims[:10]\n",
        "\n",
        "all_claims = []\n",
        "for i, analysis in enumerate(llm_analyses):\n",
        "    claims = extract_claims_from_analysis(analysis['analysis'])\n",
        "    all_claims.extend(claims)\n",
        "    print(f\"Vid√©o {i+1}: {len(claims)} affirmation(s) extraite(s)\")\n",
        "\n",
        "print(f\"\\nTotal d'affirmations √† v√©rifier: {len(all_claims)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. V√©rification des Faits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fact_checker = FactChecker()\n",
        "\n",
        "print(f\"üîç V√©rification de {len(all_claims)} affirmation(s)...\")\n",
        "fact_check_results = fact_checker.verify_claims(all_claims, language=language)\n",
        "\n",
        "print(\"\\n‚úÖ V√©rification termin√©e\")\n",
        "print(f\"\\nR√©sultats par affirmation:\")\n",
        "for claim, result in list(fact_check_results.items())[:3]:\n",
        "    print(f\"\\nüìå {claim[:60]}...\")\n",
        "    print(f\"   Verdict: {result['verdict']}\")\n",
        "    print(f\"   Score: {result['credibility_score']}%\")\n",
        "    print(f\"   Sources trouv√©es: {len(result['sources'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Compilation des R√©sultats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compiler tous les r√©sultats\n",
        "results = {\n",
        "    'metadata': {\n",
        "        'source': video_url if analysis_mode == \"video\" else f\"@{username}\",\n",
        "        'analysis_date': datetime.now().isoformat(),\n",
        "        'video_count': len(video_paths),\n",
        "        'llm_provider': llm_provider,\n",
        "        'language': language\n",
        "    },\n",
        "    'videos': []\n",
        "}\n",
        "\n",
        "# S'assurer que toutes les listes ont la m√™me longueur\n",
        "while len(video_metadata_list) < len(video_paths):\n",
        "    video_metadata_list.append({})\n",
        "\n",
        "for i, (video_path, transcription, llm_analysis, metadata) in enumerate(\n",
        "    zip(video_paths, transcriptions, llm_analyses, video_metadata_list)\n",
        "):\n",
        "    video_claims = extract_claims_from_analysis(llm_analysis['analysis'])\n",
        "    \n",
        "    video_fact_check = {}\n",
        "    if video_claims:\n",
        "        for claim in video_claims:\n",
        "            if claim in fact_check_results:\n",
        "                video_fact_check[claim] = fact_check_results[claim]\n",
        "    \n",
        "    if video_fact_check:\n",
        "        avg_score = sum(r['credibility_score'] for r in video_fact_check.values()) / len(video_fact_check)\n",
        "        verdicts = [r['verdict'] for r in video_fact_check.values()]\n",
        "        main_verdict = max(set(verdicts), key=verdicts.count) if verdicts else 'non_verifie'\n",
        "    else:\n",
        "        avg_score = 50\n",
        "        main_verdict = 'non_verifie'\n",
        "    \n",
        "    video_result = {\n",
        "        'title': metadata.get('title', video_path.stem) if metadata else video_path.stem,\n",
        "        'metadata': metadata if metadata else {},\n",
        "        'transcription': transcription,\n",
        "        'llm_analysis': llm_analysis,\n",
        "        'fact_checking': {\n",
        "            'credibility_score': int(avg_score),\n",
        "            'verdict': main_verdict,\n",
        "            'claims': video_fact_check,\n",
        "            'sources': []\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    all_sources = []\n",
        "    for claim_result in video_fact_check.values():\n",
        "        all_sources.extend(claim_result.get('sources', []))\n",
        "        all_sources.extend(claim_result.get('fact_checking_results', []))\n",
        "        all_sources.extend(claim_result.get('scientific_results', []))\n",
        "        all_sources.extend(claim_result.get('news_results', []))\n",
        "    \n",
        "    seen_urls = set()\n",
        "    unique_sources = []\n",
        "    for source in all_sources:\n",
        "        url = source.get('url', '')\n",
        "        if url and url not in seen_urls:\n",
        "            seen_urls.add(url)\n",
        "            unique_sources.append(source)\n",
        "    \n",
        "    video_result['fact_checking']['sources'] = unique_sources[:20]\n",
        "    results['videos'].append(video_result)\n",
        "\n",
        "all_scores = [v['fact_checking']['credibility_score'] for v in results['videos']]\n",
        "all_verdicts = [v['fact_checking']['verdict'] for v in results['videos']]\n",
        "\n",
        "results['statistics'] = {\n",
        "    'average_credibility': sum(all_scores) / len(all_scores) if all_scores else 0,\n",
        "    'verified_count': sum(1 for v in all_verdicts if v != 'non_verifie'),\n",
        "    'unverified_count': sum(1 for v in all_verdicts if v == 'non_verifie'),\n",
        "    'verdict_distribution': {v: all_verdicts.count(v) for v in set(all_verdicts)}\n",
        "}\n",
        "\n",
        "print(\"‚úÖ R√©sultats compil√©s\")\n",
        "print(f\"\\nüìä Statistiques:\")\n",
        "print(f\"   Score moyen: {results['statistics']['average_credibility']:.1f}%\")\n",
        "print(f\"   Vid√©os v√©rifi√©es: {results['statistics']['verified_count']}\")\n",
        "print(f\"   Vid√©os non v√©rifi√©es: {results['statistics']['unverified_count']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Visualisations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "visualizer = ResultVisualizer()\n",
        "\n",
        "print(\"üìä G√©n√©ration des visualisations...\")\n",
        "fig1 = visualizer.create_credibility_chart(results['videos'])\n",
        "plt.show()\n",
        "\n",
        "fig2 = visualizer.create_verdict_pie(results['videos'])\n",
        "plt.show()\n",
        "\n",
        "all_transcriptions = [v['transcription']['text'] for v in results['videos']]\n",
        "fig3 = visualizer.create_wordcloud(all_transcriptions)\n",
        "plt.show()\n",
        "\n",
        "fig4 = visualizer.create_timeline_chart(results['videos'])\n",
        "if fig4:\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualisations g√©n√©r√©es\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Sauvegarde des R√©sultats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "storage = ResultStorage()\n",
        "\n",
        "filename_prefix = f\"{username if analysis_mode == 'user' else 'video'}\"\n",
        "saved_files = storage.save_results(results, filename_prefix=filename_prefix)\n",
        "\n",
        "print(f\"‚úÖ R√©sultats sauvegard√©s:\")\n",
        "print(f\"   JSON: {saved_files['json']}\")\n",
        "print(f\"   Markdown: {saved_files['markdown']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Tableau de Bord Interactif\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dashboard = visualizer.create_interactive_dashboard(results['videos'])\n",
        "dashboard.show()\n",
        "\n",
        "dashboard_path = Config.OUTPUT_DIR / f\"dashboard_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html\"\n",
        "dashboard.write_html(str(dashboard_path))\n",
        "print(f\"‚úÖ Tableau de bord sauvegard√©: {dashboard_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
